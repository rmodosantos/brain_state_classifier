{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin = loadmat('Z:/ricardo/data/Project/State_classification/Reviewed_labels/brain_state_trainset.mat')\n",
    "val_origin = loadmat('Z:/ricardo/data/Project/State_classification/Reviewed_labels/brain_state_valset.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = train_origin['under_train_movement'].flatten().reshape(-1,1)\n",
    "moveScaler = RobustScaler(quantile_range=(10,90)).fit(x_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_test(data_origin,spectr_field, mov_field, mov_scalefunc):\n",
    "\n",
    "    #swap dimensions of spectrograms to have samples in the first dimension\n",
    "    #l1 = np.shape(data_origin[spectr_field])\n",
    "    #data_origin[spectr_field] = data_origin[spectr_field].reshape((l1[1],l1[0],l1[2]))\n",
    "\n",
    "    #normalize spectral data using a quantile-based method that is not sensitive to outliers\n",
    "    def normfunc(x):\n",
    "        #center and normalize data based on percentiles 10 and 90\n",
    "        transf = RobustScaler(quantile_range=(10,90)).fit(x)\n",
    "        return transf.transform(x)\n",
    "\n",
    "\n",
    "    spectral_norm = np.zeros((np.shape(data_origin[spectr_field])[1],np.shape(data_origin[spectr_field])[0],\n",
    "    np.shape(data_origin[spectr_field])[2]))\n",
    "    \n",
    "    #normalize movement\n",
    "    mov_norm = mov_scalefunc.transform(data_origin[mov_field].flatten().reshape(-1,1))\n",
    "    mov_norm = np.reshape(mov_norm,(np.shape(data_origin[mov_field])))\n",
    "    mov_norm = np.transpose(mov_norm) #transpose to have it in the form instances x movement profiles\n",
    "\n",
    "    #Convert power spectral values to logarithms to minimize sekewness\n",
    "    for i in range(np.shape(spectral_norm)[0]):\n",
    "        spec = np.log(np.squeeze(data_origin[spectr_field][:,i,:]))\n",
    "        #print(np.shape(spec))\n",
    "        spec = normfunc(spec.flatten().reshape(-1,1)) #flatten the multidimensional data for normalization\n",
    "        #spec = spec.flatten().reshape(-1,1)\n",
    "        #spec = spec.ravel()\n",
    "        #then recover the original format\n",
    "        spectral_norm[i,:,:] = np.reshape(spec,(np.shape(spectral_norm)[1],np.shape(spectral_norm)[2]))\n",
    "        if np.quantile(mov_norm[i,:],0.15)<0:\n",
    "            mov_norm[i,:] = mov_norm[i,:]-np.quantile(mov_norm[i,:],0.15)\n",
    "\n",
    "    #normalize movement data using the same approach, but now using whole dataset movement stats\n",
    "    #mov_norm = normfunc(data_origin[mov_field].flatten().reshape(-1,1))\n",
    "    #mov_norm = mov_scalefunc.transform(data_origin[mov_field].ravel())\n",
    "    #mov_norm = np.reshape(mov_norm,(np.shape(data_origin[mov_field])))\n",
    "    #mov_norm = np.transpose(mov_norm) #transpose to have it in the form instances x movement profiles\n",
    "\n",
    "    return spectral_norm, mov_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_norm_train, mov_norm_train = normalize_test(train_origin,'under_train_spectrograms','under_train_movement',moveScaler)\n",
    "spectral_norm_val, mov_norm_val = normalize_test(val_origin, 'under_val_spectrograms', 'under_val_movement',moveScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(data, spectra, mov, target_field,sampling='under'):\n",
    "        \n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "        #The oversampling function only accepts instance x features format as inputs, \n",
    "        # so we need to reshape spectral data and concatenate it with movement\n",
    "        X_data = np.concatenate((mov, spectra.reshape(len(mov),\n",
    "                np.shape(spectra)[1]*np.shape(spectra)[2])),axis=1)\n",
    "        y_data = data[target_field].ravel()\n",
    "\n",
    "        #instantiate oversampler and apply it\n",
    "        if sampling=='under':\n",
    "                ros = RandomUnderSampler(random_state=34)\n",
    "        elif sampling=='over':\n",
    "                ros = RandomOverSampler(random_state=34)\n",
    "\n",
    "        X_res, target_res = ros.fit_resample(X_data, y_data)\n",
    "\n",
    "        #recover original data formats\n",
    "        mov_norm_res = X_res[:,:np.shape(mov)[1]]\n",
    "        spectral_norm_res = np.reshape(X_res[:, np.shape(mov)[1]:],\n",
    "                (len(mov_norm_res),np.shape(spectra)[1],np.shape(spectra)[2]))\n",
    "\n",
    "        return spectral_norm_res, mov_norm_res, target_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_norm_train_res, mov_norm_train_res, target_train_res = oversample(train_origin,spectral_norm_train, mov_norm_train,'under_train_target','under')\n",
    "spectral_norm_val_res, mov_norm_val_res, target_val_res = oversample(val_origin,spectral_norm_val, mov_norm_val,'under_val_target','under')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object and random samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset class from torch allows convenient storage and sampling of the data, particularly useful for neural network training.\n",
    "First we need to create a class that will allow sampling batches of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStates(Dataset):\n",
    "    def __init__(self, movement, spec, labels):\n",
    "        self.labels = labels\n",
    "        self.movement = movement\n",
    "        self.spec = spec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.movement)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X1 = self.movement[index,:]\n",
    "        X2 = self.spec[index,:,:]\n",
    "        y = self.labels[index]\n",
    "        return X1, X2, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStatesCUDA(Dataset):\n",
    "    def __init__(self, movement, spec, labels):\n",
    "        self.labels = labels\n",
    "        self.movement = movement\n",
    "        self.spec = spec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.movement)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X1 = self.movement[index, :,:]\n",
    "        X2 = self.spec[index, :, :,:]\n",
    "        y = self.labels[index]-1\n",
    "        return X1, X2, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo\\AppData\\Local\\Temp\\ipykernel_14076\\3723264624.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spectral_norm_trainCUDA = torch.tensor(spectral_norm_train_res,dtype=torch.float)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't pin tensor constructed from numpy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Ricardo\\code\\brain_states\\notebooks\\CUDA_optimization.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Ricardo/code/brain_states/notebooks/CUDA_optimization.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m spectral_norm_trainCUDA \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(spectral_norm_train_res,dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ricardo/code/brain_states/notebooks/CUDA_optimization.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m spectral_norm_trainCUDA\u001b[39m.\u001b[39mpin_memory()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Ricardo/code/brain_states/notebooks/CUDA_optimization.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m target_trainCUDA \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(target_train_res,dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong, device\u001b[39m=\u001b[39;49mdev, pin_memory\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ricardo/code/brain_states/notebooks/CUDA_optimization.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m data_train \u001b[39m=\u001b[39m DataStates(mov_norm_train_res, spectral_norm_train_res, target_train_res)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Ricardo/code/brain_states/notebooks/CUDA_optimization.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m data_trainCUDA \u001b[39m=\u001b[39m DataStatesCUDA(mov_norm_trainCUDA, spectral_norm_trainCUDA, target_trainCUDA)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't pin tensor constructed from numpy"
     ]
    }
   ],
   "source": [
    "#Create datasets\n",
    "#move training data to GPU\n",
    "l1 = np.shape(mov_norm_train_res)\n",
    "l2 = np.shape(spectral_norm_train_res)\n",
    "\n",
    "spectral_norm_train_res = torch.tensor(spectral_norm_train_res.reshape((l2[0],1,l2[1],l2[2])))\n",
    "\n",
    "#mov_norm_trainCUDA = torch.tensor(mov_norm_train_res.reshape((l1[0],1,l1[1])),dtype=torch.float,device=dev,pin_memory=True)\n",
    "spectral_norm_trainCUDA = torch.tensor(spectral_norm_train_res,dtype=torch.float)\n",
    "spectral_norm_trainCUDA.pin_memory()\n",
    "target_trainCUDA = torch.tensor(target_train_res,dtype=torch.long, device=dev, pin_memory=True)\n",
    "data_train = DataStates(mov_norm_train_res, spectral_norm_train_res, target_train_res)\n",
    "data_trainCUDA = DataStatesCUDA(mov_norm_trainCUDA, spectral_norm_trainCUDA, target_trainCUDA)\n",
    "data_val = DataStates(mov_norm_val_res, spectral_norm_val_res, target_val_res)\n",
    "#data_test = DataStates(mov_norm_test, spectral_norm_test, test_origin['test_target'].ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_norm_trainCUDA.is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the neural network class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, device, name=None, filter_size1d=5, filter_size=5, fc1_size=120, fc2_size=84,n_conv1=6,n_convtime=3):\n",
    "        \"\"\"Define the convolutional neural network.\n",
    "        The parameters describing filter sizes and the number of neurons in the fully connected\n",
    "        layers can be provided externally.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Net,self).__init__()\n",
    "        if name:\n",
    "            self.name=name\n",
    "        \n",
    "        #define input size of first fully connected layer depending on image resolution\n",
    "        global pool1h, pool1w, pooltimew, num_conv1\n",
    "\n",
    "        wrange = torch.tensor([2,5,10,20,50],dtype=int,device=dev)\n",
    "\n",
    "        pool1h = torch.zeros(5, dtype=int, device=dev)\n",
    "        #pool2h = torch.zeros(5, dtype=int, device=dev)\n",
    "        pool1w = torch.zeros(5, dtype=int, device=dev)\n",
    "        #pool2w = torch.zeros(5, dtype=int, device=dev)\n",
    "        pooltimew = torch.zeros(5, dtype=int, device=dev)\n",
    "\n",
    "        for i, w in enumerate(wrange):\n",
    "            picres=torch.tensor([w*2+1,81], dtype=int, device=dev)\n",
    "            timeres=torch.tensor([w*2*5+1], dtype=int, device=dev)\n",
    "\n",
    "            #print(picres[0])\n",
    "            #sizes\n",
    "            pool1h[i] = picres[0]-(3-1)\n",
    "            #pool2h[i] = torch.floor(((pool1h[i]-(3-1))-2)/2)+1\n",
    "            pool1w[i] = picres[1]-(filter_size-1)\n",
    "            pooltimew[i] = torch.floor(((timeres-(filter_size1d-1))-2)/2)+1\n",
    "            #pool2w[i] = torch.floor(((pool1w[i]-(filter_size-1))-2)/2)+1\n",
    "        pool1h[4] = picres[0]-(3-1)-1\n",
    "        pooltimew[4] = pooltimew[4] - 1\n",
    "\n",
    "        num_conv1 = torch.tensor(n_conv1,dtype=int, device=dev)\n",
    "        #picres = torch.tensor([[101,81]]) #tensor with image resolution for spectral data\n",
    "\n",
    "        #pool1h = torch.floor(((picres[0][0]-(filter_size-1))-2)/2)+1 #height of 1st maxpool layer output\n",
    "        #pool2h = torch.floor(((pool1h-(filter_size-1))-2)/2)+1 #height of 2nd maxpool layer output\n",
    "        #pool1w = torch.floor(((picres[0][1]-(filter_size-1))-2)/2)+1 #width of 1st maxpool layer output\n",
    "        #pool2w = torch.floor(((pool1w-(filter_size-1))-2)/2)+1 #width of 2nd maxpool layer output\n",
    "        \n",
    "        #branch of spectral images\n",
    "        self.conv1=nn.Conv2d(1,n_conv1,(3,filter_size)) #set a fixed number of filters of 6\n",
    "        #self.pool=nn.MaxPool2d(2,2) #max-pooling parameters are fixed\n",
    "        #self.conv2=nn.Conv2d(n_conv1,n_conv2,(3,filter_size)) #fixed number of filters 16\n",
    "\n",
    "        #branch of movement\n",
    "        self.convtime = nn.Conv1d(1,n_convtime,filter_size1d, device=dev) #use 3 1d convolution filters \n",
    "        self.pooltime = nn.MaxPool1d(2,2) #max-pooling\n",
    "        #pooltimew = torch.floor(((torch.tensor(501)-(filter_size1d-1))-2)/2)+1 #width of the max-pool output\n",
    "\n",
    "        #fully connected layers\n",
    "        #the first layer gets the inputs of the 2D branch concatenated with those from the 1D branch\n",
    "        \n",
    "        self.fc1 = nn.ModuleList([])\n",
    "        self.fc2 = nn.ModuleList([])\n",
    "        self.fc3 = nn.ModuleList([])\n",
    "        for i, w in enumerate(wrange):\n",
    "            self.fc1.append(nn.Linear(int(n_conv1*pool1h[i].item()*pool1w[i].item())+int(n_convtime*pooltimew[i].item()),int(fc1_size*50/w)))\n",
    "            self.fc2.append(nn.Linear(int(fc1_size*50/w),int(fc2_size*50/w))) #second fully connected layer\n",
    "            self.fc3.append(nn.Linear(int(fc2_size*50/w),4)) #intermediate output layer\n",
    "        \n",
    "        #self.fc1=nn.Linear(int(n_conv2*pool2h.item()*pool2w.item())+int(n_convtime*pooltimew.item()),fc1_size)\n",
    "\n",
    "        \n",
    "        self.fcOut = nn.Linear(20,4) #final output layer\n",
    "        \n",
    "\n",
    "    def forward(self, x, x2):\n",
    "        #x denotes the spectral inputs and x2 denotes movement inputs\n",
    "        #the output of all convolution layers passes trhough a ReLu activation function\n",
    "        wrange = torch.tensor([2,5,10,20,50],dtype=int,device=dev)\n",
    "        xio = torch.zeros((len(x),4,5),dtype=torch.float,device=dev)\n",
    "\n",
    "        \n",
    "        for i, w in enumerate(wrange):\n",
    "            \n",
    "            #xi = x[:,:,51-w:51+w+1,:]\n",
    "            #xi2 = x2[:,:,251-w*5:251+w*5+1]\n",
    "\n",
    "            \n",
    "            xi = F.relu(self.conv1(x[:,:,51-w:51+w+1,:]))\n",
    "            #print(num_conv1, pool1h[i].item(),pool1w[i].item())\n",
    "            #print(np.shape(x))\n",
    "            #xi = self.pool(F.relu(self.conv2(xi)))\n",
    "            \n",
    "            #xi = xi.view(-1, int(num_conv1*pool1h[i].item()*pool1w[i].item()))\n",
    "            #print(xi.shape)\n",
    "            xi = xi.view(-1, xi.size(dim=1)*xi.size(dim=2)*xi.size(dim=3))\n",
    "            st = time.time()\n",
    "            xi2 = self.pooltime(F.relu(self.convtime(x2[:,:,251-w*5:251+w*5+1])))\n",
    "            en = time.time()\n",
    "            print(xi2.shape)\n",
    "            #xi2 = xi2.reshape(np.shape(xi2)[0],np.shape(xi2)[1]*np.shape(xi2)[2])\n",
    "            xi2 = xi2.reshape(xi2.size(dim=0),xi2.size(dim=1)*xi2.size(dim=2))\n",
    "            \n",
    "            #print(xi.dtype, xio.dtype)\n",
    "        #the output of the 2 fully connect hidden-layers goes through ReLu activation\n",
    "            xi = F.relu(self.fc1[i](torch.concat((xi, xi2),dim=1)))\n",
    "            xi = F.relu(self.fc2[i](xi))\n",
    "            xio[:,:,i] = self.fc3[i](xi) #intermediate output layer\n",
    "            \n",
    "            #en = time.time()\n",
    "            if i==4:\n",
    "                print((en-st)*1000)\n",
    "        \n",
    "        xf = F.softmax(self.fcOut(torch.concat((xio[:,:,0],xio[:,:,1],xio[:,:,2],xio[:,:,3],xio[:,:,4]),dim=1)),dim=1)\n",
    "        \n",
    "        \n",
    "        return xf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(net, data_train, batch_size=10, epochs=4, epoch_start=0, printr=False, save_loss=False):\n",
    "    \"\"\"Function to train the neural network\n",
    "        Inputs:\n",
    "            net: network to train\n",
    "            batch_size: size of the mini-batch used in the SGD optimization\n",
    "            epochs: number of traning epochs\n",
    "            epoch start: number of epochs run so far + 1. Used to keep track of total training\n",
    "                epochs when the function takes an already pretrained network as input\n",
    "            printr: boolean to determine whether running loss is displayed during execution\n",
    "            save_loss: boolean used to indicate wheter to store information about the running loss\"\"\"\n",
    " \n",
    "    net = net.float()\n",
    "    net.to(dev)\n",
    "\n",
    "    # create training loader\n",
    "    trainloader = DataLoader(data_train,batch_size=int(batch_size))\n",
    "\n",
    "    #train\n",
    "    import time \n",
    "    start = time.time()\n",
    "    global loss\n",
    "    print('here')\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        epoch_start += 1\n",
    "        running_loss = 0.0\n",
    "        rloss = []\n",
    "        \n",
    "        for i, sample in enumerate(trainloader):\n",
    "            #l1 = np.shape(sample[0])\n",
    "            #l2 = np.shape(sample[1])\n",
    "\n",
    "            #print(type(sample[0]),type(sample[1]))\n",
    "            #get the inputs with the approppriate shape for the network function\n",
    "            #input_mov = torch.tensor(sample[0].reshape((l1[0],1,l1[1])),dtype=torch.float,device=dev)\n",
    "            #input_spec = torch.tensor(sample[1].reshape(l2[0],1,l2[1],l2[2]),dtype=torch.float,device=dev)\n",
    "            #labels = torch.tensor(sample[2]-1, dtype=torch.long,device=dev)\n",
    "            #print(type(input_mov))\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #compute network output\n",
    "            #outputs=net(input_spec,input_mov)\n",
    "            #start2 = time.time()\n",
    "            outputs=net(sample[1],sample[0])\n",
    "            #end2 = time.time()\n",
    "            #print(end2-start2)\n",
    "            \n",
    "            #print(np.shape(sample[2]))\n",
    "            #calculate loss (criterion is a global variable initialized externaly)\n",
    "            loss = criterion(outputs, sample[2])\n",
    "            \n",
    "            #update weights based on backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if save_loss:\n",
    "                rloss.append(loss.item())\n",
    "\n",
    "            if printr:\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % 10 == 9:    # print every 10 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                        (epoch + 1, i + 1, running_loss / 10))\n",
    "                    running_loss = 0.0\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    if printr:\n",
    "        print('Finished Training')\n",
    "        print('training time ', end-start)\n",
    "    \n",
    "    if save_loss:\n",
    "        return rloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint function\n",
    "\n",
    "Function used to instantiate the intialize either a new neural network or taking a pre-saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Checkpoint(path, name=None, filter_size1d=5, filter_size=5, fc1_size=120, fc2_size=84, lr=0.0005,n_conv1=6,n_convtime=3):\n",
    "    \"\"\"Function used to instantiate the intialize either a new neural network or taking a pre-saved model\n",
    "            Inputs:\n",
    "                path: path of the file containing model information(saved with the SaveCheckpoint function\n",
    "                name: network name\n",
    "                filter_size1d, filter_size, fc1_size, fc2_size are network hyperparameters\n",
    "                lr: learning rate\"\"\"\n",
    "                \n",
    "    #instantiate neural network\n",
    "    global net,criterion,optimizer,epoch_start\n",
    "    \n",
    "    #print(filter_size1d)\n",
    "    net = Net(name, dev, filter_size1d, filter_size, fc1_size, fc2_size, n_conv1, n_convtime).float().to(dev)\n",
    "    \n",
    "    #create optimizer\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor([0.1,1,0.1,0.1],dtype=torch.float,device=dev))\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.95)\n",
    "    \n",
    "    if path == '':\n",
    "        epoch_start = 0\n",
    "        #loss = torch.zeros(1,requires_grad=True)\n",
    "    else:\n",
    "        checkpt = torch.load(path)\n",
    "        filter_size1d=int(checkpt['tuned_parameters']['filter_size1d'])\n",
    "        filter_size = int(checkpt['tuned_parameters']['filter_size'])\n",
    "        fc1_size = int(checkpt['tuned_parameters']['fc1_size'])\n",
    "        fc2_size = int(checkpt['tuned_parameters']['fc2_size'])\n",
    "        print(filter_size1d)\n",
    "        net = Net(name, filter_size1d, filter_size, fc1_size, fc2_size)\n",
    "        \n",
    "        net.load_state_dict(checkpt['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpt['optimizer_state_dict'])\n",
    "        #epoch_start = checkpt['epoch']\n",
    "        loss = checkpt['loss']\n",
    "        net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network test function\n",
    "\n",
    "Next, we define a function to run to test the predictive performance of the CNN on the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, data,batch_size=10, printr=False):\n",
    "    \"\"\"Function to test neural network predictions on a given dataset\n",
    "            Inputs:\n",
    "                net: Network to test\n",
    "                sampler: Subset random sampler used to sample validation or test sets\n",
    "                batch_size: size of batchs to compute predictions on\n",
    "                printr: Whether to show performance results\"\"\"\n",
    "\n",
    "    testloader = DataLoader(data,batch_size=batch_size)\n",
    "    global dev \n",
    "    \n",
    "    dev = torch.device(\"cpu\")\n",
    "\n",
    "    pool1h.to(torch.device('cpu')) \n",
    "    pool1w.to(torch.device('cpu'))\n",
    "    pooltimew.to(torch.device('cpu'))\n",
    "    #num_conv1.to(torch.device('cpu'))\n",
    "    net = net.float()\n",
    "    net.to(torch.device('cpu'))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for sample in testloader:\n",
    "            l1 = np.shape(sample[0])\n",
    "            l2 = np.shape(sample[1])\n",
    "            #reshaope input data to approppriate format\n",
    "            input_mov = torch.tensor(sample[0].reshape((l1[0],1,l1[1])),dtype=torch.float)\n",
    "            input_spec = torch.tensor(sample[1].reshape(l2[0],1,l2[1],l2[2]),dtype=torch.float)\n",
    "            labels = sample[2]-1\n",
    "            \n",
    "            #print(input_spec.is_cuda,input_mov.is_cuda, next(net.parameters()).is_cuda)\n",
    "            #compute predictions from the class with max output\n",
    "            outputs=net(input_spec,input_mov)\n",
    "            predicted = np.argmax(outputs,axis=1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    #compute confusion matrix\n",
    "    cm = confusion_matrix(labels,predicted)\n",
    "    \n",
    "    if printr:\n",
    "        print(cm)\n",
    "        print('Accuracy:\\nTotal:', 100*correct/total, '\\nWake:',cm[0,0]/np.sum(cm[0,:]),'\\nQuiet wake:',\n",
    "        cm[1,1]/np.sum(cm[1,:]),'\\nNREM:', cm[2,2]/np.sum(cm[2,:]),'\\nREM:', cm[3,3]/np.sum(cm[3,:]))\n",
    "    \n",
    "    dev = torch.device(\"cuda:0\")\n",
    "\n",
    "    return cm, 100*correct/total, predicted, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev=torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m Checkpoint(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNet-Capstone\u001b[39m\u001b[39m'\u001b[39m, lr\u001b[39m=\u001b[39m\u001b[39m0.002\u001b[39m, n_conv1\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m,n_convtime\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,fc1_size\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,fc2_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, filter_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, filter_size1d\u001b[39m=\u001b[39m\u001b[39m14\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m trainNet(net,data_trainCUDA, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,printr\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mUntitled-1.ipynb Cell 32\u001b[0m in \u001b[0;36mtrainNet\u001b[1;34m(net, data_train, batch_size, epochs, epoch_start, printr, save_loss)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=40'>41</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=42'>43</a>\u001b[0m \u001b[39m#compute network output\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=43'>44</a>\u001b[0m \u001b[39m#outputs=net(input_spec,input_mov)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=44'>45</a>\u001b[0m \u001b[39m#start2 = time.time()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=45'>46</a>\u001b[0m outputs\u001b[39m=\u001b[39mnet(sample[\u001b[39m1\u001b[39;49m],sample[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=46'>47</a>\u001b[0m \u001b[39m#end2 = time.time()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=47'>48</a>\u001b[0m \u001b[39m#print(end2-start2)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=48'>49</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=49'>50</a>\u001b[0m \u001b[39m#print(np.shape(sample[2]))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=50'>51</a>\u001b[0m \u001b[39m#calculate loss (criterion is a global variable initialized externaly)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=51'>52</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, sample[\u001b[39m2\u001b[39m])\n",
      "File \u001b[1;32md:\\Ricardo\\miniconda3\\envs\\statesBO\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mUntitled-1.ipynb Cell 32\u001b[0m in \u001b[0;36mNet.forward\u001b[1;34m(self, x, x2)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=75'>76</a>\u001b[0m xio \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(x),\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m),dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat,device\u001b[39m=\u001b[39mdev)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=78'>79</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, w \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(wrange):\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=79'>80</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=80'>81</a>\u001b[0m     \u001b[39m#xi = x[:,:,51-w:51+w+1,:]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=81'>82</a>\u001b[0m     \u001b[39m#xi2 = x2[:,:,251-w*5:251+w*5+1]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=84'>85</a>\u001b[0m     xi \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x[:,:,\u001b[39m51\u001b[39;49m\u001b[39m-\u001b[39;49mw:\u001b[39m51\u001b[39;49m\u001b[39m+\u001b[39;49mw\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m,:]))\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=85'>86</a>\u001b[0m     \u001b[39m#print(num_conv1, pool1h[i].item(),pool1w[i].item())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=86'>87</a>\u001b[0m     \u001b[39m#print(np.shape(x))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=87'>88</a>\u001b[0m     \u001b[39m#xi = self.pool(F.relu(self.conv2(xi)))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=88'>89</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=89'>90</a>\u001b[0m     \u001b[39m#xi = xi.view(-1, int(num_conv1*pool1h[i].item()*pool1w[i].item()))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=90'>91</a>\u001b[0m     \u001b[39m#print(xi.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#Y155sdW50aXRsZWQ%3D?line=91'>92</a>\u001b[0m     xi \u001b[39m=\u001b[39m xi\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, xi\u001b[39m.\u001b[39msize(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mxi\u001b[39m.\u001b[39msize(dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m*\u001b[39mxi\u001b[39m.\u001b[39msize(dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m))\n",
      "File \u001b[1;32md:\\Ricardo\\miniconda3\\envs\\statesBO\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Ricardo\\miniconda3\\envs\\statesBO\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32md:\\Ricardo\\miniconda3\\envs\\statesBO\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "Checkpoint('', name='Net-Capstone', lr=0.002, n_conv1=6,n_convtime=2,fc1_size=200,fc2_size=100, filter_size=10, filter_size1d=14)\n",
    "\n",
    "\n",
    "trainNet(net,data_trainCUDA, epochs=100,batch_size=100,printr=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2, 4])\n",
      "torch.Size([100, 2, 19])\n",
      "torch.Size([100, 2, 44])\n",
      "torch.Size([100, 2, 94])\n",
      "torch.Size([100, 2, 243])\n",
      "8.013725280761719\n",
      "1043.6615943908691 83.78124237060547\n"
     ]
    }
   ],
   "source": [
    "name='Net-Capstone'\n",
    "lr=0.002\n",
    "n_conv1=6\n",
    "n_convtime=2\n",
    "fc1_size=200\n",
    "fc2_size=100\n",
    "filter_size=10\n",
    "filter_size1d=14\n",
    "batch_size=100\n",
    "epochs=1\n",
    "\n",
    " #instantiate neural network\n",
    "global net,criterion,optimizer,epoch_start\n",
    "\n",
    "#print(filter_size1d)\n",
    "net = Net(name, dev, filter_size1d, filter_size, fc1_size, fc2_size, n_conv1, n_convtime).float().to(dev)\n",
    "\n",
    "#create optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([0.1,1,0.1,0.1],dtype=torch.float,device=dev))\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.95)\n",
    "\n",
    "\n",
    "# create training loader\n",
    "trainloader = DataLoader(data_trainCUDA,batch_size=batch_size)\n",
    "\n",
    "#train\n",
    "#start = time.time()\n",
    "sample = torch.tensor([],dtype=float,device=dev)\n",
    "sampleS = spectral_norm_trainCUDA[:100,:,:,:]\n",
    "sampleM = mov_norm_trainCUDA[:100,:,:]\n",
    "sampleL = target_trainCUDA[:100]\n",
    "w=torch.tensor(50,dtype=int,device=dev)\n",
    "\n",
    "convtime = nn.Conv1d(1,n_convtime,filter_size1d,device=dev) #use 3 1d convolution filters \n",
    "pooltime = nn.MaxPool1d(2,2) #max-pooling\n",
    "\n",
    "start = time.time()\n",
    "pooltime(F.relu(convtime(sampleM[:,:,251-w*5:251+w*5+1])))\n",
    "end = time.time()\n",
    "\n",
    "start2 = time.time()\n",
    "outputs=net(sampleS,sampleM)\n",
    "end2 = time.time()\n",
    "\n",
    "print((end-start)*1000, (end2-start2)*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0035037994384766\n",
      "13.018369674682617\n",
      "5.979776382446289\n",
      "7.813692092895508\n",
      "6.980419158935547\n",
      "0.6020069122314453\n",
      "1.9943714141845703\n",
      "2.345561981201172\n",
      "1.9996166229248047\n",
      "8.974075317382812\n"
     ]
    }
   ],
   "source": [
    "wrange = torch.tensor([2,5,10,20,50],dtype=int,device=dev)\n",
    "\n",
    "pool1h = torch.zeros(5, dtype=int, device=dev)\n",
    "#pool2h = torch.zeros(5, dtype=int, device=dev)\n",
    "pool1w = torch.zeros(5, dtype=int, device=dev)\n",
    "#pool2w = torch.zeros(5, dtype=int, device=dev)\n",
    "pooltimew = torch.zeros(5, dtype=int, device=dev)\n",
    "\n",
    "for i, w in enumerate(wrange):\n",
    "    picres=torch.tensor([w*2+1,81], dtype=int, device=dev)\n",
    "    timeres=torch.tensor([w*2*5+1], dtype=int, device=dev)\n",
    "\n",
    "    #print(picres[0])\n",
    "    #sizes\n",
    "    pool1h[i] = picres[0]-(3-1)\n",
    "    #pool2h[i] = torch.floor(((pool1h[i]-(3-1))-2)/2)+1\n",
    "    pool1w[i] = picres[1]-(filter_size-1)\n",
    "    pooltimew[i] = torch.floor(((timeres-(filter_size1d-1))-2)/2)+1\n",
    "    #pool2w[i] = torch.floor(((pool1w[i]-(filter_size-1))-2)/2)+1\n",
    "pool1h[4] = picres[0]-(3-1)-1\n",
    "pooltimew[4] = pooltimew[4] - 1\n",
    "\n",
    "num_conv1 = torch.tensor(n_conv1,dtype=int, device=dev)\n",
    "\n",
    "#branch of spectral images\n",
    "conv1=nn.Conv2d(1,n_conv1,(3,filter_size),device=dev) #set a fixed number of filters of 6\n",
    "\n",
    "\n",
    "#branch of movement\n",
    "convtime = nn.Conv1d(1,n_convtime,filter_size1d, device=dev) #use 3 1d convolution filters \n",
    "pooltime = nn.MaxPool1d(2,2) #max-pooling\n",
    "\n",
    "\n",
    "fc1 = nn.ModuleList([])\n",
    "fc2 = nn.ModuleList([])\n",
    "fc3 = nn.ModuleList([])\n",
    "for i, w in enumerate(wrange):\n",
    "    fc1.append(nn.Linear(int(n_conv1*pool1h[i].item()*pool1w[i].item())+int(n_convtime*pooltimew[i].item()),int(fc1_size*50/w)))\n",
    "    fc2.append(nn.Linear(int(fc1_size*50/w),int(fc2_size*50/w))) #second fully connected layer\n",
    "    fc3.append(nn.Linear(int(fc2_size*50/w),4)) #intermediate output layer\n",
    "\n",
    "fc1.to(dev)\n",
    "fc2.to(dev)\n",
    "fc3.to(dev)\n",
    "\n",
    "\n",
    "fcOut = nn.Linear(20,4) #final output layer\n",
    "\n",
    "\n",
    "def forward(x, x2, conv1,convtime,pooltime):\n",
    "    #x denotes the spectral inputs and x2 denotes movement inputs\n",
    "    #the output of all convolution layers passes trhough a ReLu activation function\n",
    "    xio = torch.zeros((len(x),4,5),dtype=torch.float,device=dev)\n",
    "\n",
    "    for i, w in enumerate(wrange):\n",
    "        #print(w)\n",
    "        st = time.time()\n",
    "        xt = pooltime(F.relu(convtime(x2[:,:,251-w*5:251+w*5+1])))\n",
    "        torch.cuda.current_stream().synchronize()\n",
    "        en = time.time()\n",
    "        print((en-st)*1000)\n",
    "        xt = xt.reshape(xt.size(dim=0),xt.size(dim=1)*xt.size(dim=2))\n",
    "        xt2 = F.relu(conv1(x[:,:,51-w:51+w+1,:]))\n",
    "        \n",
    "        #print(xi.shape)\n",
    "        xt2 = xt2.view(-1, xt2.size(dim=1)*xt2.size(dim=2)*xt2.size(dim=3))\n",
    "        xt2 = F.relu(fc1[i](torch.concat((xt2, xt),dim=1)))\n",
    "        xt2 = F.relu(fc2[i](xt2))\n",
    "\n",
    "    for i, w in enumerate(wrange):\n",
    "        \n",
    "        #print('here')\n",
    "        \n",
    "        xi = F.relu(conv1(x[:,:,51-w:51+w+1,:]))\n",
    "        \n",
    "        #print(xi.shape)\n",
    "        xi = xi.view(-1, xi.size(dim=1)*xi.size(dim=2)*xi.size(dim=3))\n",
    "        st = time.time()\n",
    "        xi2 = pooltime(F.relu(convtime(x2[:,:,251-w*5:251+w*5+1])))\n",
    "        torch.cuda.current_stream().synchronize()\n",
    "        en = time.time()\n",
    "        print((en-st)*1000)\n",
    "        #xi2 = xi2.reshape(np.shape(xi2)[0],np.shape(xi2)[1]*np.shape(xi2)[2])\n",
    "        xi2 = xi2.reshape(xi2.size(dim=0),xi2.size(dim=1)*xi2.size(dim=2))\n",
    "        \n",
    "        #print(xi.is_cuda, xi2.is_cuda, xio.is_cuda)\n",
    "    #the output of the 2 fully connect hidden-layers goes through ReLu activation\n",
    "        xi = F.relu(fc1[i](torch.concat((xi, xi2),dim=1)))\n",
    "        xi = F.relu(fc2[i](xi))\n",
    "        xio[:,:,i] = fc3[i](xi) #intermediate output layer\n",
    "        \n",
    "        #en = time.time()\n",
    "        #if i==4:\n",
    "        #    print((en-st)*1000)\n",
    "\n",
    "    #xf = F.softmax(fcOut(torch.concat((xio[:,:,0],xio[:,:,1],xio[:,:,2],xio[:,:,3],xio[:,:,4]),dim=1)),dim=1)\n",
    "\n",
    "    #return xf\n",
    "\n",
    "forward(sampleS,sampleM,conv1,convtime,pooltime)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    epoch_start += 1\n",
    "    running_loss = 0.0\n",
    "    rloss = []\n",
    "    \n",
    "    #sample=iter(trainloader)\n",
    "    #sample = sample.next()\n",
    "\n",
    "\n",
    "    #l1 = np.shape(sample[0])\n",
    "    #l2 = np.shape(sample[1])\n",
    "\n",
    "    #print(type(sample[0]),type(sample[1]))\n",
    "    #get the inputs with the approppriate shape for the network function\n",
    "    #input_mov = torch.tensor(sample[0].reshape((l1[0],1,l1[1])),dtype=torch.float,device=dev)\n",
    "    #input_spec = torch.tensor(sample[1].reshape(l2[0],1,l2[1],l2[2]),dtype=torch.float,device=dev)\n",
    "    #labels = torch.tensor(sample[2]-1, dtype=torch.long,device=dev)\n",
    "    #print(type(input_mov))\n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #compute network output\n",
    "    #outputs=net(input_spec,input_mov)\n",
    "    #start2 = time.time()\n",
    "    #outputs=net(sample[1],sample[0])\n",
    "    outputs=net(sampleS,sampleM)\n",
    "    #end2 = time.time()\n",
    "    #print(end2-start2)\n",
    "    \n",
    "    #print(np.shape(sample[2]))\n",
    "    #calculate loss (criterion is a global variable initialized externaly)\n",
    "    #loss = criterion(outputs, sample[2])\n",
    "    loss = criterion(outputs, sampleL)\n",
    "\n",
    "    #update weights based on backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #if save_loss:\n",
    "    #    rloss.append(loss.item())\n",
    "\n",
    "    # if printr:\n",
    "    #     # print statistics\n",
    "    #     running_loss += loss.item()\n",
    "    #     if i % 10 == 9:    # print every 10 mini-batches\n",
    "    #         print('[%d, %5d] loss: %.3f' %\n",
    "    #             (epoch + 1, i + 1, running_loss / 10))\n",
    "    #         running_loss = 0.0\n",
    "\n",
    "#end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('statesBO')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fddedfb9a392c877be259f48c491e51bd47af3b6e5eb67bebc6d911b49879b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
